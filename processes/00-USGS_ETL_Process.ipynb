{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Since December 28, 2019, Puerto Rico has experienced a highly active seismic season. Since then, the lives of Puerto Ricans who live in the southern and southwestern regions of the island have not been the same. I personally experienced a few of those during my winter vacation in Puerto Rico. Since then, I constantly check the earthquake app and the family group chat, especially in the middle of the night just to know that everyone is safe.\n",
    "\n",
    "After all this, I decided to start looking for data. I was curious about how many events we had before, their intensity, frequency, highest magnitude, etc. This repo and all the steps below will help me answer those questions and continue to monitor the Seismic activity in my beloved Puerto Rico Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all needed libraries\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, exc\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pymysql\n",
    "import getpass\n",
    "import sys\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL (Extract, Transform, Load) from USGS \n",
    "\n",
    "## Data Extract\n",
    "\n",
    "I extracted the data by using the query method with geojson format of the USGS earthquake API. \n",
    "\n",
    "You can find more details about this API here:\n",
    "* https://earthquake.usgs.gov/fdsnws/event/1/\n",
    "\n",
    "### API Parameters:\n",
    "\n",
    "#### Coordinates:\n",
    "\n",
    "According to the Puerto Rico Seismic Network, the coordinates used for Puerto Rico's Region Boundary are the following:\n",
    "\n",
    "* maxlongitude = -63.5\n",
    "* minlongitude = -69\n",
    "* minlatitude = 17\n",
    "* maxlatitude = 20\n",
    "\n",
    "![Puerto Rico Region](http://redsismica.uprm.edu/Spanish/Informe_Sismo/css/mapa_caribenor.jpg)\n",
    "\n",
    "\n",
    "#### Dates:\n",
    "To perform the initial load and updates we will used the following parameters as well:\n",
    "\n",
    "* starttime\n",
    "* endtime\n",
    "* updatedafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function was created to extract the data:\n",
    "\n",
    "def get_eq_data(url):\n",
    "  #verify if request is good\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # If the response was successful, no Exception will be raised and data will be extracted\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')  \n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        print('Successful Request!')    \n",
    "        print(url)\n",
    "        #extract data\n",
    "        req = requests.get(url)\n",
    "        eq_data = req.json()\n",
    "        #get record count\n",
    "        rec_cnt = eq_data['metadata']['count']\n",
    "        print('Record count = ', rec_cnt)\n",
    "        if rec_cnt == 0:\n",
    "            sys.exit('No new data to update available...')\n",
    "        else:\n",
    "            #save data we care about into a dataframe:\n",
    "            eq_data = eq_data['features']\n",
    "            eq_list = []  #initializing list of dictionaries\n",
    "            for x in (list(range(0, rec_cnt))):            \n",
    "                #getting event properties in a dictionary format\n",
    "                eq_dict = {'id': eq_data[x]['id'] ,\n",
    "                           'time': eq_data[x]['properties']['time'],\n",
    "                           'updated': eq_data[x]['properties']['updated'],\n",
    "                           'title': eq_data[x]['properties']['title'],\n",
    "                           'mag': eq_data[x]['properties']['mag'], \n",
    "                           'magType': eq_data[x]['properties']['magType'],            \n",
    "                           'lon': eq_data[x]['geometry']['coordinates'][0],\n",
    "                           'lat': eq_data[x]['geometry']['coordinates'][1],\n",
    "                           'depth': eq_data[x]['geometry']['coordinates'][2],\n",
    "                           'sources': eq_data[x]['properties']['sources'],\n",
    "                           'url': eq_data[x]['properties']['url'],\n",
    "                           'region':None}\n",
    "                eq_list.append(eq_dict)   \n",
    "                #save data into dataframe\n",
    "            df = pd.DataFrame(eq_list)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquake Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this data useful for my requirements, the following changes will be applied:\n",
    "    \n",
    "* change dates to datetime format\n",
    "* normalize magType to have all records in upper case\n",
    "* set mag values to numeric \n",
    "* get the DataFrame to a GeoDataFrame format to identify the seismic region for each earthquake event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function was created to clean and transform the extracted data:\n",
    "\n",
    "def clean_eq_data(df):\n",
    "    #update time from unix timestamps to datetime\n",
    "    df['date_time'] = pd.to_datetime(pd.to_datetime(df.time, unit='ms',origin='unix'\n",
    "                                      ).apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    df['updated_datetime'] = pd.to_datetime(pd.to_datetime(df.updated, unit='ms',origin='unix'\n",
    "                                             ).apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    #magtype normalizing\n",
    "    df.magType = df.magType.str.upper()\n",
    "    df.mag = pd.to_numeric(df.mag)\n",
    "    #removing unnecesary columns\n",
    "    df.drop(columns = ['time','updated'], inplace = True)\n",
    "#     #changing to a GeoDataFrame to create geometry series\n",
    "#     gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon,df.lat))\n",
    "#     return gdf   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up MySQL connections and tables\n",
    "\n",
    "All the data collected for this solution will be saved in a MySQL database. A table named **eq_data** will contain all earthquake data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for database connection\n",
    "def connect_to_db(db,user):\n",
    "    try:\n",
    "        engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                           .format(user=user,\n",
    "                           pw= getpass.getpass('Enter database password:'),\n",
    "                           db=db))\n",
    "        conn = engine.connect()\n",
    "        print(\"Successfull connection to MySQL database\")\n",
    "    except exc.DBAPIError as err:\n",
    "        print(\"Connection error to database\")\n",
    "        raise\n",
    "    return engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter database password:········\n",
      "Successfull connection to MySQL database\n"
     ]
    }
   ],
   "source": [
    "#Connect to database\n",
    "db = \"eq_pr\"\n",
    "user = 'root'\n",
    "conn = connect_to_db(db, user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create table for Earthquake Data \n",
    "conn.execute('''CREATE TABLE `eq_pr`.`eq_data` (\n",
    "  `id` VARCHAR(45) NOT NULL,\n",
    "  `title` VARCHAR(100),\n",
    "  `mag` FLOAT ,\n",
    "  `magType` VARCHAR(10) ,\n",
    "  `sources` VARCHAR(45) ,\n",
    "  `url` VARCHAR(100) ,\n",
    "  `date_time` DATETIME NOT NULL,\n",
    "  `updated_datetime` DATETIME NOT NULL,\n",
    "  `lat` FLOAT NOT NULL,\n",
    "  `lon` FLOAT NULL,\n",
    "  `depth` FLOAT ,\n",
    "  `region` VARCHAR(45) NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  INDEX `id` (`id` ASC) VISIBLE);\n",
    "''').rowcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving region data in MySQL table\n",
    "\n",
    "Region data was generated by using the image from http://redsismica.uprm.edu/Spanish/educacion/terremotos/sismicidad.php and imported in Google Earth as an image overlay, then draw the polygons for each one of the 28 seismic regions for Puerto Rico area. The polygons were saved in an .kml file.\n",
    "\n",
    "#### **Puerto Rico Seismic Regions**\n",
    "\n",
    "![Puerto Rico Seismic Regions](https://miro.medium.com/max/1050/0*E9LbqmjeFPFf9-gj.png)\n",
    "\n",
    "\n",
    "#### **Polygons created with Google Earth**\n",
    "\n",
    "![Polygons created with Google Earth](https://miro.medium.com/max/1050/1*03ngUcNKHLOJdIwfd-d7HA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about this process as well as the PIP analysis in my blog post: https://medium.com/analytics-vidhya/point-in-polygon-analysis-using-python-geopandas-27ea67888bff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading kml file and saving the polygon into a MySQL table\n",
    "\n",
    "#Enabling the read and write functionalities for KML-driver\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "\n",
    "# Filepath to KML file\n",
    "fp = \"./data/PR_Seismic_Regions.kml\"\n",
    "\n",
    "# Create GeoDataFrame for Region data\n",
    "polys = gpd.read_file(fp, driver='KML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South of Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>POLYGON Z ((-66.00365 17.96593 0.00000, -66.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Southwestern Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>POLYGON Z ((-66.99953 17.97945 0.00000, -66.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>POLYGON Z ((-66.99952 17.97944 0.00000, -67.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southeastern Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>POLYGON Z ((-66.00114 17.99906 0.00000, -66.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Central Puerto Rico</td>\n",
       "      <td></td>\n",
       "      <td>POLYGON Z ((-67.01198 18.39215 0.00000, -67.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name Description  \\\n",
       "0      South of Puerto Rico               \n",
       "1  Southwestern Puerto Rico               \n",
       "2      Southern Puerto Rico               \n",
       "3  Southeastern Puerto Rico               \n",
       "4       Central Puerto Rico               \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON Z ((-66.00365 17.96593 0.00000, -66.03...  \n",
       "1  POLYGON Z ((-66.99953 17.97945 0.00000, -66.99...  \n",
       "2  POLYGON Z ((-66.99952 17.97944 0.00000, -67.00...  \n",
       "3  POLYGON Z ((-66.00114 17.99906 0.00000, -66.00...  \n",
       "4  POLYGON Z ((-67.01198 18.39215 0.00000, -67.00...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the polygon data\n",
    "polys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 28 entries, 0 to 27\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   Name         28 non-null     object  \n",
      " 1   Description  28 non-null     object  \n",
      " 2   geometry     28 non-null     geometry\n",
      "dtypes: geometry(1), object(2)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "polys.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-67.35044436009619 17.560448042890084 1.967024205601362 0.478332323525116\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,35.59922840930528)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.039340484112027244\" opacity=\"0.6\" d=\"M -66.00364731113208,17.96592761805959 L -66.03286168699188,17.94889524332291 L -66.05628897269642,17.93401033942753 L -66.10191276731771,17.90417134968824 L -66.10891491193593,17.88258078230613 L -66.12071711584683,17.8477100509052 L -66.17918068124575,17.82240195358468 L -66.23194316380447,17.83318620074911 L -66.25199720329515,17.83359355513404 L -66.29763348436771,17.83397353855259 L -66.32630956899499,17.83544145938664 L -66.33460005197772,17.83585966350893 L -66.34602886647859,17.83648718223549 L -66.35633834959809,17.83668858131403 L -66.36799505978011,17.83731407401951 L -66.38774554647286,17.83830286964757 L -66.42616539262669,17.83908908963693 L -66.47502370753631,17.83528504546366 L -66.53522155732696,17.8463868559145 L -66.56686762762061,17.8790826258788 L -66.57078253809533,17.93084783699578 L -66.65349751820966,17.93787074553927 L -66.67856830220259,17.93271345182055 L -66.69929782458766,17.94157429969464 L -66.73941696052529,17.94344710073513 L -66.74061290617814,17.94350404140835 L -66.74139974166407,17.94354023495976 L -66.77199827921358,17.94496896681718 L -66.83217545792621,17.93528539883372 L -66.86517365662806,17.92493453721742 L -66.87990903805472,17.92239832108169 L -66.88176776912721,17.92208016050911 L -66.88549317467658,17.9214391770714 L -66.89042115806659,17.92057233949679 L -66.89293208204997,17.92012766748082 L -66.92495532986199,17.91440350990666 L -66.94573832222387,17.91012190974395 L -66.96279325321343,17.90883027894843 L -67.00049456456148,17.89806161232122 L -67.0305584134397,17.90104464322535 L -67.0410788961798,17.90208947061564 L -67.08211992408336,17.91059529985888 L -67.08816390731069,17.90938792177035 L -67.09417974627048,17.9096533641288 L -67.10494606016522,17.90885798930414 L -67.12883146526981,17.91571271004475 L -67.13354000623802,17.94105372496847 L -67.13446392893167,17.94605509192202 L -67.14078943650213,17.94688136953205 L -67.14355824991392,17.94742252812726 L -67.14876408192134,17.9487836232626 L -67.15968609903794,17.950064130333 L -67.16746739546835,17.95010314278908 L -67.17316491757579,17.94612239509167 L -67.17417671356259,17.94196928800671 L -67.1734531701128,17.93478433500037 L -67.178237895085,17.92837770914238 L -67.18417732851746,17.92335153916001 L -67.18772856504144,17.92602233978704 L -67.19194196823423,17.9287993784266 L -67.19768995417836,17.93312059601018 L -67.20241562068186,17.9411311141786 L -67.21199989295847,17.93908308485608 L -67.2271505510122,17.93041860153412 L -67.24686827785254,17.91832812884298 L -67.26214997479188,17.90927098030117 L -67.27759161174059,17.90018526681039 L -67.27586603785524,17.89636977272142 L -67.27311993620653,17.88917040519263 L -67.26427793971429,17.86530372363175 L -67.26193202221266,17.8526401076144 L -67.25892186334829,17.83681143176855 L -67.25755256043865,17.82906359604022 L -67.2569706476167,17.82581690064064 L -67.25441130990978,17.81206485464249 L -67.24851779555289,17.78459659718942 L -67.24270299382817,17.75741637740736 L -67.23642354562138,17.72823344609369 L -67.22792365964598,17.68824632924619 L -67.21697066218758,17.66050778986873 L -67.21647762558595,17.6624912336864 L -67.09049697180305,17.6636161452067 L -67.00295220154064,17.66416654227095 L -66.8971164521878,17.66329785383938 L -66.80518856588634,17.66082737256603 L -66.72711761808327,17.65970000158202 L -66.64603486252541,17.65853819332133 L -66.56957640305676,17.65731524850812 L -66.50843508649353,17.6603867895357 L -66.4610517669553,17.66048892320847 L -66.39226861643544,17.65769271332464 L -66.34030459555883,17.6592165654607 L -66.31737040306896,17.6548669341645 L -66.2072955884338,17.65346442041914 L -66.08339211404595,17.65487630374634 L -66.0298151807517,17.65044597647488 L -66.00684121480356,17.64020556761781 L -65.9746798866766,17.64307253783585 L -65.95936300871934,17.64888100202696 L -65.89942284912422,17.64814156945354 L -65.83282396730955,17.64368383184016 L -65.74578586186507,17.6454815353363 L -65.65472402198202,17.64328462646396 L -65.55281623472052,17.63700767131657 L -65.45627290285043,17.63330079124569 L -65.48848256412556,17.67259113561689 L -65.5193266270404,17.71183735174278 L -65.55818210140039,17.76410381384962 L -65.6065620762667,17.81643684436892 L -65.64669353908987,17.86612383222466 L -65.6491691257496,17.86931268953077 L -65.6557346134107,17.87890999054914 L -65.68609659934954,17.92260981627229 L -65.72610965512303,17.91702844139874 L -65.76655822426972,17.91216418994701 L -65.78413284299052,17.92544866697249 L -65.81047637564346,17.92668788038304 L -65.85455776499435,17.92877926033733 L -65.87108545867561,17.9290428603596 L -65.92397269606732,17.93019783445312 L -65.99242007493069,17.9313284180323 L -66.00364731113208,17.96592761805959 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x1614e80bb08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polys.geometry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melissa\\anaconda3\\lib\\site-packages\\geopandas\\geodataframe.py:852: UserWarning: Geometry column does not contain geometry.\n",
      "  warnings.warn(\"Geometry column does not contain geometry.\")\n"
     ]
    }
   ],
   "source": [
    "#Cleaning region data to be able to send it to MySQL\n",
    "regions = polys.copy()\n",
    "\n",
    "regions['geometry'] = regions['geometry'].apply(lambda x: str(x).\n",
    "                                                replace('Z ','').\n",
    "                                                replace('0,',',').\n",
    "                                                replace('POLYGON ','POLYGON').replace('0))','))'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melissa\\anaconda3\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\reflection.py:193: SAWarning: Did not recognize type 'polygon' of column 'geom'\n",
      "  \"Did not recognize type '%s' of column '%s'\" % (type_, name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x1614d9eb7c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving data into MySQL\n",
    "regions.to_sql('pr_regions', con=conn, schema='eq_pr',\n",
    "               if_exists='replace', index=False)\n",
    "\n",
    "#Reformatting table to set as GIS data\n",
    "conn.execute('''ALTER TABLE `eq_pr`.`pr_regions` \n",
    "                ADD COLUMN `geom` Polygon;''')\n",
    "\n",
    "conn.execute('''ALTER TABLE `eq_pr`.`pr_regions` \n",
    "                DROP COLUMN `Description`;''')\n",
    "\n",
    "conn.execute('''UPDATE `eq_pr`.`pr_regions`\n",
    "                SET geom =  ST_GeomFromText(geometry) ;''')\n",
    "\n",
    "# conn.execute('''ALTER TABLE `eq_pr`.`pr_regions` \n",
    "#                 ADD COLUMN `centroid` Point;''')\n",
    "\n",
    "# conn.execute('''UPDATE `eq_pr`.`pr_regions`  \n",
    "#                 SET centroid =  ST_Centroid(ST_GeomFromText(geometry));''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data load\n",
    "\n",
    "The API has a limit of 20k records per request, we will proceed to do the initial data load in batches, then we will create a process to update the data with the latests events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to load data into the eq_data table\n",
    "\n",
    "def load_to_db(df, conn):\n",
    "    #creating table to stage eq_data\n",
    "    df.to_sql('eq_stage', con = conn, if_exists = 'replace', chunksize = 1000)\n",
    "    #get counts before changes\n",
    "    s_count = conn.execute('''SELECT COUNT(id) FROM eq_data;''').fetchone()[0]\n",
    "    #remove records from eq_data table that exists in eq_stage table\n",
    "    conn.execute ('''DELETE FROM eq_data  WHERE id IN ( SELECT * FROM\n",
    "                    (SELECT s.id FROM eq_stage s\n",
    "                    INNER JOIN eq_data d ON s.id=d.id\n",
    "                    WHERE s.updated_datetime<>d.updated_datetime) AS t); ''')\n",
    "    r_count = conn.execute('''SELECT COUNT(id) FROM eq_data;''').fetchone()[0]\n",
    "    #load new and updated records into eq_data from eq_stage data\n",
    "    conn.execute ('''INSERT INTO eq_data ( id, title, mag, magType, sources, url, date_time, updated_datetime, \n",
    "                  depth, lat, lon, region) SELECT id, title, mag, magType, sources, url, date_time, updated_datetime, \n",
    "                  depth, lat, lon, region FROM eq_stage where id NOT IN (SELECT id FROM eq_data);''')\n",
    "    e_count = conn.execute('''SELECT COUNT(id) FROM eq_data;''').fetchone()[0]\n",
    "    delta = e_count-s_count\n",
    "    if delta == 0:\n",
    "        print('No records to update')\n",
    "    else:\n",
    "        print('Records before load:', s_count)\n",
    "        print('Updated records:', delta)\n",
    "        print('Final record count:', e_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract, clean data and perform initial loads to MySQL table:\n",
    "\n",
    "def etl_i_data(minlat, maxlat, minlon, maxlon, sd, ed,conn):\n",
    "    #define url\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&minlatitude='+ minlat +'&maxlatitude='+ maxlat + '&minlongitude=' + minlon + '&maxlongitude=' + maxlon + '&starttime=' + sd + '&endtime='+ ed\n",
    "#     #connect to db\n",
    "#     conn = connect_to_db(db,user)  \n",
    "    #extract data with json request\n",
    "    edf = get_eq_data(url)\n",
    "    #clean data\n",
    "    eq_df = clean_eq_data(edf)\n",
    "    #load it in MySQL\n",
    "    load_to_db(eq_df, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PR boundaries:\n",
    "minlat = '17'\n",
    "maxlat = '20'\n",
    "minlon = '-69'\n",
    "maxlon = '-63.5'\n",
    "#start and end dates:\n",
    "sd = '1600-01-01T00:00:00'\n",
    "ed = '2014-01-01T23:59:59'\n",
    "etl_i_data(minlat, maxlat, minlon, maxlon, sd, ed, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = '2014-01-02T00:00:00'\n",
    "ed = '2019-01-01T23:59:59'\n",
    "etl_i_data(minlat, maxlat, minlon, maxlon, sd, ed, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = '2019-01-02T00:00:00'\n",
    "ed = '2020-09-10T23:59:59'\n",
    "etl_i_data(minlat, maxlat, minlon, maxlon, sd, ed, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-In-Polygon (PIP) for Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function that will retrieve from MySQL the region polygon data to be used to identify the region for each event\n",
    "def get_regions(conn):\n",
    "    data = conn.execute(''' select name, geometry from pr_regions;''').fetchall()\n",
    "    #Clean geometry series to transform dataframe to a geoDataFrame\n",
    "    r = pd.DataFrame(data,columns=['Name','geometry'] )\n",
    "    r.geometry = r.geometry.apply(lambda x: list(eval(x.replace('POLYGON((', '('\n",
    "                                                   ).replace(' ))', ')'\n",
    "                                                            ).replace( ' , ', '),('\n",
    "                                                                     ).replace(' ',','))))\n",
    "    r.geometry = r.geometry.apply(lambda x: Polygon(x))\n",
    "    regions = gpd.GeoDataFrame(r, crs=\"EPSG:4326\")\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function that will perform the PIP analysis and will assign a region for each event\n",
    "def get_intersections (gdf,conn):\n",
    "    regions = get_regions(conn)\n",
    "    r_list = list(regions.Name)\n",
    "    #create empty dataframe\n",
    "    df = pd.DataFrame().reindex_like(gdf).dropna()\n",
    "    for r in r_list:\n",
    "        #get geometry for specific region\n",
    "        pol = (regions.loc[regions.Name==r])\n",
    "        pol.reset_index(drop = True, inplace = True)\n",
    "        #identify those records from gdf that are intersecting with the region polygon\n",
    "        pip_mask = gdf.within(pol.loc[0, 'geometry'])\n",
    "        #filter gdf to keep only the intersecting records\n",
    "        pip_data = gdf.loc[pip_mask].copy()\n",
    "        #create a new column and assign the region name as the value\n",
    "        pip_data['region']= r\n",
    "        #append region data to empty dataframe\n",
    "        df = df.append(pip_data)\n",
    "    print('Validating region assignment...')\n",
    "    if df.loc[df.id.duplicated() == True].shape[0] > 0:\n",
    "        print(\"There are id's with more than one region\")\n",
    "    elif gdf.loc[~gdf.id.isin(df.id)].shape[0] > 0:\n",
    "        print(\"There are id's without an assigned region\")\n",
    "    else:\n",
    "        print(\"No issues with region assignmet!\")\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df = df.drop(columns='geometry')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data initially loaded \n",
    "eq_data = pd.read_sql_query(\"SELECT id, lat, lon, region FROM eq_data WHERE region IS Null;\", conn)\n",
    "eq_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create GeoDataFrame\n",
    "eq_gdf = gpd.GeoDataFrame(\n",
    "    eq_data, geometry=gpd.points_from_xy(eq_data.lon,eq_data.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_intersections(eq_gdf,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data with regions into MySQL\n",
    "df.to_sql('regions_stage', con = conn, if_exists = 'replace', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update regions records into eq_data from eq_stage data\n",
    "conn.execute ('''UPDATE eq_data e , regions_stage r\n",
    "                SET e.region =r.region\n",
    "                where e.id = r.id;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying all events have an assigned region\n",
    "pd.read_sql_query(\"SELECT id, lat, lon, region FROM eq_data WHERE region IS Null;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM eq_data ORDER BY date_time DESC LIMIT 50;\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update process going foward:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get this whole process automated and simplified, the following functions have been created or updated:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated to return a GeoDataFrame\n",
    "\n",
    "def clean_eq_data(df):\n",
    "    #update time from unix timestamps to datetime\n",
    "    df['date_time'] = pd.to_datetime(pd.to_datetime(df.time, unit='ms',origin='unix'\n",
    "                                      ).apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    df['updated_datetime'] = pd.to_datetime(pd.to_datetime(df.updated, unit='ms',origin='unix'\n",
    "                                             ).apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    #magtype normalizing\n",
    "    df.magType = df.magType.str.upper()\n",
    "    df.mag = pd.to_numeric(df.mag)\n",
    "    #removing unnecesary columns\n",
    "    df.drop(columns = ['time','updated'], inplace = True)\n",
    "    #changing to a GeoDataFrame to create geometry series\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon,df.lat))\n",
    "    return gdf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to use to load new events and update reviewed events\n",
    "\n",
    "def delta_eq_data(conn):\n",
    "    #static parameters for this solution\n",
    "    minlat = '17'\n",
    "    maxlat = '20'\n",
    "    minlon = '-69'\n",
    "    maxlon = '-63.5'\n",
    "    ud = conn.execute('''Select max(updated_datetime) from eq_data''').fetchone()[0]\n",
    "    ud = ud - datetime.timedelta(7)\n",
    "    ud = ud.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    #define url for request with updatedafter\n",
    "    url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&minlatitude='+ minlat +'&maxlatitude='+ maxlat + '&minlongitude=' + minlon + '&maxlongitude=' + maxlon + '&updatedafter=' + ud\n",
    "    df = get_eq_data(url)\n",
    "    #clean data & change to gdf\n",
    "    eq_df = clean_eq_data(df)\n",
    "    #run intersection\n",
    "    print('Assigning regions...')\n",
    "    gdf = get_intersections (eq_df,conn)\n",
    "    #load data into MySQL\n",
    "    print('Loading data into MySQL...')\n",
    "    load_to_db(gdf, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta_eq_data(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
